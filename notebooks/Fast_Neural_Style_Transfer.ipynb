import tensorflow as tf
from tensorflow.keras.applications import vgg19
from tensorflow.keras.models import Model
import numpy as np
from PIL import Image
import time

# Load and preprocess images
def load_and_process_image(image_path, target_size):
    img = Image.open(image_path)
    img = img.resize(target_size)
    img_array = np.array(img)
    img_array = vgg19.preprocess_input(img_array)
    return tf.convert_to_tensor(np.expand_dims(img_array, axis=0))

# Deprocess image to convert back to viewable format
def deprocess_image(img_array):
    img_array = img_array.copy()
    if len(img_array.shape) == 4:
        img_array = img_array[0]
    img_array[:, :, 0] += 103.939  # Revert VGG19 preprocessing
    img_array[:, :, 1] += 116.779
    img_array[:, :, 2] += 123.679
    img_array = img_array[:, :, ::-1]  # BGR to RGB
    img_array = np.clip(img_array, 0, 255).astype('uint8')
    return Image.fromarray(img_array)

# Define layers for content and style extraction
content_layers = ['block5_conv2']
style_layers = [
    'block1_conv1',
    'block2_conv1',
    'block3_conv1',
    'block4_conv1',
    'block5_conv1'
]

# Build VGG19 model
vgg = vgg19.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False
model_outputs = [vgg.get_layer(layer).output for layer in (content_layers + style_layers)]
model = Model(vgg.input, model_outputs)

# Compute Gram matrix for style representation
def gram_matrix(input_tensor):
    channels = int(input_tensor.shape[-1])
    a = tf.reshape(input_tensor, [-1, channels])
    n = tf.cast(tf.shape(a)[0], tf.float32)
    return tf.matmul(a, a, transpose_a=True) / n

# Load content and style images
content_image = load_and_process_image('content.jpg', (512, 512))
style_image = load_and_process_image('style.jpg', (512, 512))

# Extract features and compute targets
def get_features(image):
    outputs = model(image)
    content_features = outputs[:len(content_layers)]
    style_features = outputs[len(content_layers):]
    return content_features, style_features

content_targets, _ = get_features(content_image)
_, style_outputs = get_features(style_image)
style_targets = [gram_matrix(style_feature) for style_feature in style_outputs]

# Initialize generated image with content image
generated_image = tf.Variable(content_image)

# Training parameters
content_weight = 1e4
style_weight = 1e-2
optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)

# Training step with style and content loss
@tf.function
def train_step(image):
    with tf.GradientTape() as tape:
        content_features, style_features = get_features(image)
        
        # Content loss
        content_loss = tf.add_n([
            tf.reduce_mean(tf.square(content_features[i] - content_targets[i]))
            for i in range(len(content_targets))
        ]) * content_weight / len(content_targets)
        
        # Style loss
        style_loss = tf.add_n([
            tf.reduce_mean(tf.square(gram_matrix(style_features[i]) - style_targets[i]))
            for i in range(len(style_targets))
        ]) * style_weight / len(style_targets)
        
        total_loss = content_loss + style_loss
    
    gradients = tape.gradient(total_loss, image)
    optimizer.apply_gradients([(gradients, image)])

# Run training loop
epochs = 10
steps_per_epoch = 100

for epoch in range(epochs):
    start = time.time()
    for _ in range(steps_per_epoch):
        train_step(generated_image)
    print(f"Epoch {epoch+1} completed in {time.time() - start:.2f}s")
    
    # Save intermediate result
    img = deprocess_image(generated_image.numpy())
    img.save(f'output_epoch_{epoch+1}.jpg')

# Save final result
final_image = deprocess_image(generated_image.numpy())
final_image.save('final_output.jpg')
