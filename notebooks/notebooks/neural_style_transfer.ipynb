from pathlib import Path

# Sample code to be saved as a Jupyter Notebook
notebook_content = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¨ Neural Style Transfer\n",
                "\n",
                "Apply artistic styles to photographs using TensorFlow and VGG19."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import PIL.Image\n",
                "from tensorflow.keras.applications import vgg19\n",
                "from tensorflow.keras.models import Model\n",
                "from tensorflow.keras.preprocessing import image as kp_image\n",
                "from tensorflow.keras import backend as K"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "def load_img(path_to_img):\n",
                "    max_dim = 512\n",
                "    img = PIL.Image.open(path_to_img)\n",
                "    long = max(img.size)\n",
                "    scale = max_dim / long\n",
                "    img = img.resize((round(img.size[0] * scale), round(img.size[1] * scale)), PIL.Image.Resampling.LANCZOS)\n",
                "    img = kp_image.img_to_array(img)\n",
                "    img = np.expand_dims(img, axis=0)\n",
                "    return vgg19.preprocess_input(img)\n",
                "\n",
                "def deprocess_img(processed_img):\n",
                "    x = processed_img.copy()\n",
                "    x[:, :, 0] += 103.939\n",
                "    x[:, :, 1] += 116.779\n",
                "    x[:, :, 2] += 123.68\n",
                "    x = x[:, :, ::-1]\n",
                "    x = np.clip(x, 0, 255).astype(\"uint8\")\n",
                "    return x"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "def get_content_loss(base_content, target):\n",
                "    return tf.reduce_mean(tf.square(base_content - target))\n",
                "\n",
                "def gram_matrix(input_tensor):\n",
                "    result = tf.linalg.einsum(\"bijc,bijd->bcd\", input_tensor, input_tensor)\n",
                "    input_shape = tf.shape(input_tensor)\n",
                "    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32)\n",
                "    return result / num_locations\n",
                "\n",
                "def get_style_loss(base_style, gram_target):\n",
                "    gram_style = gram_matrix(base_style)\n",
                "    return tf.reduce_mean(tf.square(gram_style - gram_target))"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "def get_model():\n",
                "    vgg = vgg19.VGG19(include_top=False, weights=\"imagenet\")\n",
                "    vgg.trainable = False\n",
                "    content_layers = [\"block5_conv2\"]\n",
                "    style_layers = [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\n",
                "    outputs = [vgg.get_layer(name).output for name in style_layers + content_layers]\n",
                "    model = Model([vgg.input], outputs)\n",
                "    return model, style_layers, content_layers"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
                "    style_weight, content_weight = loss_weights\n",
                "    model_outputs = model(init_image)\n",
                "    style_output_features = model_outputs[:len(gram_style_features)]\n",
                "    content_output_features = model_outputs[len(gram_style_features):]\n",
                "\n",
                "    style_score = tf.add_n([get_style_loss(style_output, gram_style)\n",
                "                            for style_output, gram_style in zip(style_output_features, gram_style_features)])\n",
                "    content_score = tf.add_n([get_content_loss(content_output, target_content)\n",
                "                              for content_output, target_content in zip(content_output_features, content_features)])\n",
                "\n",
                "    style_score *= style_weight\n",
                "    content_score *= content_weight\n",
                "    loss = style_score + content_score\n",
                "    return loss"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "@tf.function()\n",
                "def compute_grads(cfg):\n",
                "    with tf.GradientTape() as tape:\n",
                "        all_loss = compute_loss(**cfg)\n",
                "    return tape.gradient(all_loss, cfg[\"init_image\"]), all_loss"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "def run_style_transfer(content_path, style_path, num_iterations=1000, content_weight=1e3, style_weight=1e-2):\n",
                "    model, style_layers, content_layers = get_model()\n",
                "    content_image = load_img(content_path)\n",
                "    style_image = load_img(style_path)\n",
                "\n",
                "    style_features = [model(style_image)[i] for i in range(len(style_layers))]\n",
                "    content_features = [model(content_image)[len(style_layers):][i] for i in range(len(content_layers))]\n",
                "    gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
                "\n",
                "    init_image = tf.Variable(content_image, dtype=tf.float32)\n",
                "    optimizer = tf.optimizers.Adam(learning_rate=5.0)\n",
                "    best_loss, best_img = float(\"inf\"), None\n",
                "    loss_weights = (style_weight, content_weight)\n",
                "\n",
                "    cfg = {\n",
                "        \"model\": model,\n",
                "        \"loss_weights\": loss_weights,\n",
                "        \"init_image\": init_image,\n",
                "        \"gram_style_features\": gram_style_features,\n",
                "        \"content_features\": content_features\n",
                "    }\n",
                "\n",
                "    for i in range(num_iterations):\n",
                "        grads, loss = compute_grads(cfg)\n",
                "        optimizer.apply_gradients([(grads, init_image)])\n",
                "        clipped = tf.clip_by_value(init_image, -128.0, 128.0)\n",
                "        init_image.assign(clipped)\n",
                "\n",
                "        if loss < best_loss:\n",
                "            best_loss = loss\n",
                "            best_img = init_image.numpy()\n",
                "\n",
                "        if i % 100 == 0:\n",
                "            print(f\"Iteration {i}, Loss: {loss.numpy():.4f}\")\n",
                "\n",
                "    return deprocess_img(best_img[0])"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "execution_count": None,
            "outputs": [],
            "source": [
                "# Example usage:\n",
                "# content_path = 'your_content_image.jpg'\n",
                "# style_path = 'your_style_image.jpg'\n",
                "# result = run_style_transfer(content_path, style_path, num_iterations=500)\n",
                "# plt.imshow(result)\n",
                "# plt.title(\"Stylized Image\")\n",
                "# plt.axis('off')\n",
                "# plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

import json

# Save the notebook to a file
output_path = "/mnt/data/Neural_Style_Transfer.ipynb"
with open(output_path, "w") as f:
    json.dump(notebook_content, f)

output_path
